{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "9Cvjos6GZ9p0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN6vFJspZ9p3",
        "outputId": "cc0209f5-cab5-4b32-8308-59bace5c63ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "WJ8OsEpjZ9p4"
      },
      "outputs": [],
      "source": [
        "with open(\"C:/Users/User/Documents/Machine Learning/social listening project/tf_idf.csv\", encoding=\"utf-8\") as f:\n",
        "    # đọc file vector, đưa về dạng array[float]\n",
        "    reader = csv.reader(f)\n",
        "    data_list = [row for row in reader]\n",
        "    data_list = np.asarray(data_list[0:5500],dtype=np.float64)\n",
        "    data_list = np.concatenate((np.ones((data_list.shape[0],1)), data_list), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "YgCWIwGYZ9p5"
      },
      "outputs": [],
      "source": [
        "with open(\"C:/Users/User/Documents/Machine Learning/social listening project/Data dùng được.csv\", encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    label_list = [row for row in reader]\n",
        "    label_list = label_list[0:5500]\n",
        "    # 0 = Ukraina and 1 = Nga\n",
        "    for i in range(0,len(label_list)):\n",
        "        if label_list[i][1]==\"U\":\n",
        "            label_list[i]=0\n",
        "        else: \n",
        "            label_list[i]=1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ZT0n4ak8Z9p6"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data_list, label_list, train_size=5000/5500,test_size=500/5500, random_state=0)\n",
        "X_train = torch.tensor(X_train,dtype=torch.float64)\n",
        "X_test = torch.tensor(X_test,dtype=torch.float64)\n",
        "y_train = torch.tensor(y_train,dtype=torch.float64)\n",
        "y_test = torch.tensor(y_test,dtype=torch.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "PvJTcUr4Z9p6"
      },
      "outputs": [],
      "source": [
        "def sigmoid(s):\n",
        "    return (1/(1 + 1e-6 + torch.exp(-s)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "LBa-Iz5_Z9p7"
      },
      "outputs": [],
      "source": [
        "def loss_function(w,X_train,y_train):\n",
        "    z = sigmoid(torch.matmul(w,X_train.T))\n",
        "    loss = torch.sum(-(y_train*torch.log(z)+(1-y_train)*torch.log(1-z)))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_tensor(mini_batch,batch_size):\n",
        "    X_list, y_list = mini_batch\n",
        "    X_mini = X_list[0].reshape(1,-1)\n",
        "    y_mini = y_list[0].reshape(1)\n",
        "    for i in range(1,batch_size):\n",
        "        X_mini = torch.cat((X_mini,X_list[i].reshape(1,-1)),0)\n",
        "        y_mini = torch.cat((y_mini,y_list[i].reshape(1)),0)\n",
        "    return X_mini, y_mini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_mini_batches(X_train, y_train, batch_size, N):\n",
        "    mini_batches = []\n",
        "    # xáo thứ tự training set\n",
        "    mix_id = torch.randperm(N)\n",
        "    n_minibatches = N // batch_size\n",
        "    i = 0\n",
        "    # lấy các mini_batchs chẵn\n",
        "    for i in range(n_minibatches + 1):\n",
        "        mini_batch = mix_id[i * batch_size:(i + 1)*batch_size]\n",
        "        X_mini = [X_train[j] for j in mini_batch]\n",
        "        y_mini = [y_train[j] for j in mini_batch]\n",
        "        mini_batches.append((X_mini, y_mini))\n",
        "    # lấy một mini_batch gồm các data còn lại\n",
        "    if N % batch_size != 0:\n",
        "        mini_batch = mix_id[i * batch_size + 1:]\n",
        "        X_mini = [X_train[j] for j in mini_batch]\n",
        "        y_mini = [y_train[j] for j in mini_batch]\n",
        "        mini_batches.append((X_mini, y_mini))\n",
        "    mini_batches.pop()\n",
        "    return mini_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "Z6qL26PiZ9p8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def logistic_sigmoid_regression(X_train, y_train, w_init, eta, batch_size, tol = 1e-4, max_count = 50000):\n",
        "    w = [w_init]    # vector trọng số\n",
        "    #it = 0\n",
        "    N = X_train.shape[0]  # số data\n",
        "    count = 0   # đến số vong lặp\n",
        "    check_w_after = 100  # check lại w sau mỗi 20 lần lặp\n",
        "    loss_list = [0]\n",
        "    count_list = []\n",
        "    num_batch = N//batch_size\n",
        "    if num_batch*batch_size<N:\n",
        "        num_batch+=1\n",
        "    while count<max_count:\n",
        "        mini_batches = create_mini_batches(X_train, y_train, batch_size, N)\n",
        "        for i in range(num_batch):\n",
        "            X_mini, y_mini = make_tensor(mini_batches[i],batch_size)\n",
        "            z_mini = sigmoid(torch.matmul(w[-1],X_mini.T))\n",
        "            w_new = w[-1] + eta*torch.matmul((y_mini - z_mini),X_mini)\n",
        "            count += 1\n",
        "              \n",
        "            if count%check_w_after == 0:   \n",
        "                print(count,loss_list[-1])  \n",
        "                loss_list.append(loss_function(w_new,X_train,y_train))  \n",
        "                count_list.append(count)\n",
        "                if abs(loss_list[-1]-loss_list[-2])<tol:\n",
        "                    return w[-1],loss_list,count_list\n",
        "            w.append(w_new)\n",
        "    return w[-1],loss_list,count_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfWot0d9Z9p8",
        "outputId": "2cd89299-404f-4b71-f92a-301693416dbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100 0\n",
            "200 tensor(14209.0642, dtype=torch.float64)\n",
            "300 tensor(13480.4311, dtype=torch.float64)\n",
            "400 tensor(12904.2111, dtype=torch.float64)\n",
            "500 tensor(12355.7072, dtype=torch.float64)\n",
            "600 tensor(11881.6047, dtype=torch.float64)\n",
            "700 tensor(11386.0626, dtype=torch.float64)\n",
            "800 tensor(10929.7190, dtype=torch.float64)\n",
            "900 tensor(10502.3366, dtype=torch.float64)\n",
            "1000 tensor(10078.2245, dtype=torch.float64)\n",
            "1100 tensor(9694.9400, dtype=torch.float64)\n",
            "1200 tensor(9307.9381, dtype=torch.float64)\n",
            "1300 tensor(8993.3121, dtype=torch.float64)\n",
            "1400 tensor(8675.5530, dtype=torch.float64)\n",
            "1500 tensor(8354.2816, dtype=torch.float64)\n",
            "1600 tensor(8056.7283, dtype=torch.float64)\n",
            "1700 tensor(7787.9252, dtype=torch.float64)\n",
            "1800 tensor(7536.0029, dtype=torch.float64)\n",
            "1900 tensor(7272.3998, dtype=torch.float64)\n",
            "2000 tensor(7047.1261, dtype=torch.float64)\n",
            "2100 tensor(6826.9327, dtype=torch.float64)\n",
            "2200 tensor(6630.9420, dtype=torch.float64)\n",
            "2300 tensor(6410.0119, dtype=torch.float64)\n",
            "2400 tensor(6237.0052, dtype=torch.float64)\n",
            "2500 tensor(6052.5908, dtype=torch.float64)\n",
            "2600 tensor(5873.1812, dtype=torch.float64)\n",
            "2700 tensor(5723.2181, dtype=torch.float64)\n",
            "2800 tensor(5537.7066, dtype=torch.float64)\n",
            "2900 tensor(5388.5780, dtype=torch.float64)\n",
            "3000 tensor(5230.4382, dtype=torch.float64)\n",
            "3100 tensor(5097.1268, dtype=torch.float64)\n",
            "3200 tensor(4982.0141, dtype=torch.float64)\n",
            "3300 tensor(4844.6813, dtype=torch.float64)\n",
            "3400 tensor(4711.3165, dtype=torch.float64)\n",
            "3500 tensor(4597.0715, dtype=torch.float64)\n",
            "3600 tensor(4477.7646, dtype=torch.float64)\n",
            "3700 tensor(4376.8156, dtype=torch.float64)\n",
            "3800 tensor(4269.7769, dtype=torch.float64)\n",
            "3900 tensor(4160.5841, dtype=torch.float64)\n",
            "4000 tensor(4070.5043, dtype=torch.float64)\n",
            "4100 tensor(3972.8798, dtype=torch.float64)\n",
            "4200 tensor(3889.1574, dtype=torch.float64)\n",
            "4300 tensor(3805.1447, dtype=torch.float64)\n",
            "4400 tensor(3730.9168, dtype=torch.float64)\n",
            "4500 tensor(3649.6242, dtype=torch.float64)\n",
            "4600 tensor(3572.3954, dtype=torch.float64)\n",
            "4700 tensor(3507.7168, dtype=torch.float64)\n",
            "4800 tensor(3438.4792, dtype=torch.float64)\n",
            "4900 tensor(3368.6523, dtype=torch.float64)\n",
            "5000 tensor(3306.7073, dtype=torch.float64)\n",
            "5100 tensor(3244.9793, dtype=torch.float64)\n",
            "5200 tensor(3187.9985, dtype=torch.float64)\n",
            "5300 tensor(3125.8315, dtype=torch.float64)\n",
            "5400 tensor(3074.6426, dtype=torch.float64)\n",
            "5500 tensor(3018.5998, dtype=torch.float64)\n",
            "5600 tensor(2964.8496, dtype=torch.float64)\n",
            "5700 tensor(2923.4016, dtype=torch.float64)\n",
            "5800 tensor(2869.9896, dtype=torch.float64)\n",
            "5900 tensor(2822.3311, dtype=torch.float64)\n",
            "6000 tensor(2771.8055, dtype=torch.float64)\n",
            "6100 tensor(2729.3337, dtype=torch.float64)\n",
            "6200 tensor(2691.4130, dtype=torch.float64)\n",
            "6300 tensor(2654.4398, dtype=torch.float64)\n",
            "6400 tensor(2603.2437, dtype=torch.float64)\n",
            "6500 tensor(2571.1912, dtype=torch.float64)\n",
            "6600 tensor(2527.1221, dtype=torch.float64)\n",
            "6700 tensor(2491.4062, dtype=torch.float64)\n",
            "6800 tensor(2459.4642, dtype=torch.float64)\n",
            "6900 tensor(2417.9546, dtype=torch.float64)\n",
            "7000 tensor(2386.1949, dtype=torch.float64)\n",
            "7100 tensor(2351.4422, dtype=torch.float64)\n",
            "7200 tensor(2321.4662, dtype=torch.float64)\n",
            "7300 tensor(2289.0477, dtype=torch.float64)\n",
            "7400 tensor(2259.3365, dtype=torch.float64)\n",
            "7500 tensor(2231.3683, dtype=torch.float64)\n",
            "7600 tensor(2201.4501, dtype=torch.float64)\n",
            "7700 tensor(2174.8285, dtype=torch.float64)\n",
            "7800 tensor(2148.6055, dtype=torch.float64)\n",
            "7900 tensor(2126.0494, dtype=torch.float64)\n",
            "8000 tensor(2099.6128, dtype=torch.float64)\n",
            "8100 tensor(2073.6833, dtype=torch.float64)\n",
            "8200 tensor(2049.5982, dtype=torch.float64)\n",
            "8300 tensor(2024.2275, dtype=torch.float64)\n",
            "8400 tensor(2000.0795, dtype=torch.float64)\n",
            "8500 tensor(1978.2225, dtype=torch.float64)\n",
            "8600 tensor(1953.6964, dtype=torch.float64)\n",
            "8700 tensor(1934.9474, dtype=torch.float64)\n",
            "8800 tensor(1912.6019, dtype=torch.float64)\n",
            "8900 tensor(1890.6456, dtype=torch.float64)\n",
            "9000 tensor(1867.7040, dtype=torch.float64)\n",
            "9100 tensor(1848.3812, dtype=torch.float64)\n",
            "9200 tensor(1830.7163, dtype=torch.float64)\n",
            "9300 tensor(1811.5131, dtype=torch.float64)\n",
            "9400 tensor(1794.3811, dtype=torch.float64)\n",
            "9500 tensor(1775.6614, dtype=torch.float64)\n",
            "9600 tensor(1755.1936, dtype=torch.float64)\n",
            "9700 tensor(1739.8484, dtype=torch.float64)\n",
            "9800 tensor(1723.1090, dtype=torch.float64)\n",
            "9900 tensor(1705.9117, dtype=torch.float64)\n",
            "10000 tensor(1691.1612, dtype=torch.float64)\n",
            "10100 tensor(1671.4876, dtype=torch.float64)\n",
            "10200 tensor(1657.3663, dtype=torch.float64)\n",
            "10300 tensor(1643.4985, dtype=torch.float64)\n",
            "10400 tensor(1626.2060, dtype=torch.float64)\n",
            "10500 tensor(1612.2696, dtype=torch.float64)\n",
            "10600 tensor(1595.9494, dtype=torch.float64)\n",
            "10700 tensor(1584.1067, dtype=torch.float64)\n",
            "10800 tensor(1568.7724, dtype=torch.float64)\n",
            "10900 tensor(1556.0919, dtype=torch.float64)\n",
            "11000 tensor(1545.4293, dtype=torch.float64)\n",
            "11100 tensor(1526.7962, dtype=torch.float64)\n",
            "11200 tensor(1516.5278, dtype=torch.float64)\n",
            "11300 tensor(1504.3308, dtype=torch.float64)\n",
            "11400 tensor(1490.8662, dtype=torch.float64)\n",
            "11500 tensor(1477.9824, dtype=torch.float64)\n",
            "11600 tensor(1464.1829, dtype=torch.float64)\n",
            "11700 tensor(1454.0837, dtype=torch.float64)\n",
            "11800 tensor(1442.1821, dtype=torch.float64)\n",
            "11900 tensor(1430.3000, dtype=torch.float64)\n",
            "12000 tensor(1418.6975, dtype=torch.float64)\n",
            "12100 tensor(1406.8093, dtype=torch.float64)\n",
            "12200 tensor(1399.8270, dtype=torch.float64)\n",
            "12300 tensor(1387.7540, dtype=torch.float64)\n",
            "12400 tensor(1374.9289, dtype=torch.float64)\n",
            "12500 tensor(1366.4269, dtype=torch.float64)\n",
            "12600 tensor(1354.1070, dtype=torch.float64)\n",
            "12700 tensor(1346.4258, dtype=torch.float64)\n",
            "12800 tensor(1337.0975, dtype=torch.float64)\n",
            "12900 tensor(1325.1467, dtype=torch.float64)\n",
            "13000 tensor(1316.1815, dtype=torch.float64)\n",
            "13100 tensor(1304.4830, dtype=torch.float64)\n",
            "13200 tensor(1295.6789, dtype=torch.float64)\n",
            "13300 tensor(1286.8327, dtype=torch.float64)\n",
            "13400 tensor(1280.4282, dtype=torch.float64)\n",
            "13500 tensor(1269.3356, dtype=torch.float64)\n",
            "13600 tensor(1259.5799, dtype=torch.float64)\n",
            "13700 tensor(1253.0741, dtype=torch.float64)\n",
            "13800 tensor(1243.5836, dtype=torch.float64)\n",
            "13900 tensor(1235.8401, dtype=torch.float64)\n",
            "14000 tensor(1227.9141, dtype=torch.float64)\n",
            "14100 tensor(1217.7679, dtype=torch.float64)\n",
            "14200 tensor(1210.7119, dtype=torch.float64)\n",
            "14300 tensor(1202.1555, dtype=torch.float64)\n",
            "14400 tensor(1195.5734, dtype=torch.float64)\n",
            "14500 tensor(1186.9555, dtype=torch.float64)\n",
            "14600 tensor(1178.1488, dtype=torch.float64)\n",
            "14700 tensor(1171.3291, dtype=torch.float64)\n",
            "14800 tensor(1164.1663, dtype=torch.float64)\n",
            "14900 tensor(1158.6595, dtype=torch.float64)\n",
            "15000 tensor(1149.5466, dtype=torch.float64)\n",
            "15100 tensor(1141.9688, dtype=torch.float64)\n",
            "15200 tensor(1136.5830, dtype=torch.float64)\n",
            "15300 tensor(1129.4926, dtype=torch.float64)\n",
            "15400 tensor(1122.3312, dtype=torch.float64)\n",
            "15500 tensor(1116.3384, dtype=torch.float64)\n",
            "15600 tensor(1109.0943, dtype=torch.float64)\n",
            "15700 tensor(1103.6548, dtype=torch.float64)\n",
            "15800 tensor(1096.7265, dtype=torch.float64)\n",
            "15900 tensor(1089.8253, dtype=torch.float64)\n",
            "16000 tensor(1084.2941, dtype=torch.float64)\n",
            "16100 tensor(1076.9242, dtype=torch.float64)\n",
            "16200 tensor(1072.6180, dtype=torch.float64)\n",
            "16300 tensor(1067.1848, dtype=torch.float64)\n",
            "16400 tensor(1062.7559, dtype=torch.float64)\n",
            "16500 tensor(1055.6872, dtype=torch.float64)\n",
            "16600 tensor(1048.1559, dtype=torch.float64)\n",
            "16700 tensor(1044.0287, dtype=torch.float64)\n",
            "16800 tensor(1037.9504, dtype=torch.float64)\n",
            "16900 tensor(1033.6061, dtype=torch.float64)\n",
            "17000 tensor(1027.1480, dtype=torch.float64)\n",
            "17100 tensor(1021.9565, dtype=torch.float64)\n",
            "17200 tensor(1016.4903, dtype=torch.float64)\n",
            "17300 tensor(1011.3086, dtype=torch.float64)\n",
            "17400 tensor(1006.7255, dtype=torch.float64)\n",
            "17500 tensor(1002.0903, dtype=torch.float64)\n",
            "17600 tensor(997.6118, dtype=torch.float64)\n",
            "17700 tensor(992.7264, dtype=torch.float64)\n",
            "17800 tensor(986.6220, dtype=torch.float64)\n",
            "17900 tensor(982.0643, dtype=torch.float64)\n",
            "18000 tensor(977.6075, dtype=torch.float64)\n",
            "18100 tensor(971.8525, dtype=torch.float64)\n",
            "18200 tensor(968.0856, dtype=torch.float64)\n",
            "18300 tensor(963.6584, dtype=torch.float64)\n",
            "18400 tensor(958.0867, dtype=torch.float64)\n",
            "18500 tensor(954.2790, dtype=torch.float64)\n",
            "18600 tensor(949.2029, dtype=torch.float64)\n",
            "18700 tensor(946.2064, dtype=torch.float64)\n",
            "18800 tensor(941.5006, dtype=torch.float64)\n",
            "18900 tensor(936.9298, dtype=torch.float64)\n",
            "19000 tensor(933.3954, dtype=torch.float64)\n",
            "19100 tensor(927.9417, dtype=torch.float64)\n",
            "19200 tensor(925.5084, dtype=torch.float64)\n",
            "19300 tensor(920.1470, dtype=torch.float64)\n",
            "19400 tensor(916.6398, dtype=torch.float64)\n",
            "19500 tensor(913.1005, dtype=torch.float64)\n",
            "19600 tensor(907.8040, dtype=torch.float64)\n",
            "19700 tensor(905.3693, dtype=torch.float64)\n",
            "19800 tensor(901.1426, dtype=torch.float64)\n",
            "19900 tensor(896.8085, dtype=torch.float64)\n",
            "20000 tensor(894.0487, dtype=torch.float64)\n",
            "20100 tensor(890.8070, dtype=torch.float64)\n",
            "20200 tensor(886.4030, dtype=torch.float64)\n",
            "20300 tensor(882.3676, dtype=torch.float64)\n",
            "20400 tensor(879.3185, dtype=torch.float64)\n",
            "20500 tensor(875.3595, dtype=torch.float64)\n",
            "20600 tensor(871.4182, dtype=torch.float64)\n",
            "20700 tensor(868.7677, dtype=torch.float64)\n",
            "20800 tensor(864.8534, dtype=torch.float64)\n",
            "20900 tensor(861.4048, dtype=torch.float64)\n",
            "21000 tensor(858.8885, dtype=torch.float64)\n",
            "21100 tensor(854.2914, dtype=torch.float64)\n",
            "21200 tensor(852.3691, dtype=torch.float64)\n",
            "21300 tensor(848.9744, dtype=torch.float64)\n",
            "21400 tensor(846.4840, dtype=torch.float64)\n",
            "21500 tensor(842.3777, dtype=torch.float64)\n",
            "21600 tensor(838.0678, dtype=torch.float64)\n",
            "21700 tensor(835.6193, dtype=torch.float64)\n",
            "21800 tensor(832.5320, dtype=torch.float64)\n",
            "21900 tensor(830.7750, dtype=torch.float64)\n",
            "22000 tensor(826.6571, dtype=torch.float64)\n",
            "22100 tensor(823.4128, dtype=torch.float64)\n",
            "22200 tensor(821.4706, dtype=torch.float64)\n",
            "22300 tensor(817.4800, dtype=torch.float64)\n",
            "22400 tensor(816.4869, dtype=torch.float64)\n",
            "22500 tensor(812.0458, dtype=torch.float64)\n",
            "22600 tensor(809.0371, dtype=torch.float64)\n",
            "22700 tensor(806.8772, dtype=torch.float64)\n",
            "22800 tensor(803.5413, dtype=torch.float64)\n",
            "22900 tensor(800.6204, dtype=torch.float64)\n",
            "23000 tensor(798.2924, dtype=torch.float64)\n",
            "23100 tensor(794.3678, dtype=torch.float64)\n",
            "23200 tensor(792.2690, dtype=torch.float64)\n",
            "23300 tensor(789.7622, dtype=torch.float64)\n",
            "23400 tensor(787.2445, dtype=torch.float64)\n",
            "23500 tensor(784.4248, dtype=torch.float64)\n",
            "23600 tensor(781.2858, dtype=torch.float64)\n",
            "23700 tensor(780.1307, dtype=torch.float64)\n",
            "23800 tensor(777.5896, dtype=torch.float64)\n",
            "23900 tensor(774.0735, dtype=torch.float64)\n",
            "24000 tensor(771.8209, dtype=torch.float64)\n",
            "24100 tensor(768.3226, dtype=torch.float64)\n",
            "24200 tensor(766.8179, dtype=torch.float64)\n",
            "24300 tensor(763.8768, dtype=torch.float64)\n",
            "24400 tensor(761.5393, dtype=torch.float64)\n",
            "24500 tensor(759.6329, dtype=torch.float64)\n",
            "24600 tensor(756.4351, dtype=torch.float64)\n",
            "24700 tensor(755.3857, dtype=torch.float64)\n",
            "24800 tensor(752.0651, dtype=torch.float64)\n",
            "24900 tensor(750.1204, dtype=torch.float64)\n",
            "25000 tensor(747.5605, dtype=torch.float64)\n",
            "25100 tensor(744.5719, dtype=torch.float64)\n",
            "25200 tensor(742.8804, dtype=torch.float64)\n",
            "25300 tensor(740.7435, dtype=torch.float64)\n",
            "25400 tensor(738.3620, dtype=torch.float64)\n",
            "25500 tensor(736.6109, dtype=torch.float64)\n",
            "25600 tensor(733.7955, dtype=torch.float64)\n",
            "25700 tensor(732.3080, dtype=torch.float64)\n",
            "25800 tensor(729.5405, dtype=torch.float64)\n",
            "25900 tensor(727.8235, dtype=torch.float64)\n",
            "26000 tensor(725.3328, dtype=torch.float64)\n",
            "26100 tensor(722.9863, dtype=torch.float64)\n",
            "26200 tensor(721.3886, dtype=torch.float64)\n",
            "26300 tensor(719.2117, dtype=torch.float64)\n",
            "26400 tensor(716.9144, dtype=torch.float64)\n",
            "26500 tensor(715.4469, dtype=torch.float64)\n",
            "26600 tensor(712.4200, dtype=torch.float64)\n",
            "26700 tensor(711.1768, dtype=torch.float64)\n",
            "26800 tensor(709.2124, dtype=torch.float64)\n",
            "26900 tensor(706.8884, dtype=torch.float64)\n",
            "27000 tensor(705.3719, dtype=torch.float64)\n",
            "27100 tensor(702.3608, dtype=torch.float64)\n",
            "27200 tensor(701.3521, dtype=torch.float64)\n",
            "27300 tensor(698.7498, dtype=torch.float64)\n",
            "27400 tensor(697.4137, dtype=torch.float64)\n",
            "27500 tensor(695.1675, dtype=torch.float64)\n",
            "27600 tensor(692.9618, dtype=torch.float64)\n",
            "27700 tensor(692.3104, dtype=torch.float64)\n",
            "27800 tensor(689.7373, dtype=torch.float64)\n",
            "27900 tensor(688.6020, dtype=torch.float64)\n",
            "28000 tensor(686.4066, dtype=torch.float64)\n",
            "28100 tensor(683.7126, dtype=torch.float64)\n",
            "28200 tensor(682.8195, dtype=torch.float64)\n",
            "28300 tensor(680.6252, dtype=torch.float64)\n",
            "28400 tensor(678.7816, dtype=torch.float64)\n",
            "28500 tensor(677.1348, dtype=torch.float64)\n",
            "28600 tensor(674.8525, dtype=torch.float64)\n",
            "28700 tensor(674.0053, dtype=torch.float64)\n",
            "28800 tensor(671.7623, dtype=torch.float64)\n",
            "28900 tensor(670.3337, dtype=torch.float64)\n",
            "29000 tensor(669.5101, dtype=torch.float64)\n",
            "29100 tensor(666.1580, dtype=torch.float64)\n",
            "29200 tensor(665.5275, dtype=torch.float64)\n",
            "29300 tensor(663.2680, dtype=torch.float64)\n",
            "29400 tensor(661.3584, dtype=torch.float64)\n",
            "29500 tensor(659.9106, dtype=torch.float64)\n",
            "29600 tensor(657.8893, dtype=torch.float64)\n",
            "29700 tensor(657.0720, dtype=torch.float64)\n",
            "29800 tensor(655.3174, dtype=torch.float64)\n",
            "29900 tensor(654.2286, dtype=torch.float64)\n",
            "30000 tensor(652.3075, dtype=torch.float64)\n",
            "30100 tensor(649.8383, dtype=torch.float64)\n",
            "30200 tensor(648.7861, dtype=torch.float64)\n",
            "30300 tensor(647.5071, dtype=torch.float64)\n",
            "30400 tensor(645.7887, dtype=torch.float64)\n",
            "30500 tensor(644.1534, dtype=torch.float64)\n",
            "30600 tensor(642.1729, dtype=torch.float64)\n",
            "30700 tensor(641.5939, dtype=torch.float64)\n",
            "30800 tensor(639.3910, dtype=torch.float64)\n",
            "30900 tensor(638.0738, dtype=torch.float64)\n",
            "31000 tensor(636.5715, dtype=torch.float64)\n",
            "31100 tensor(634.6861, dtype=torch.float64)\n",
            "31200 tensor(633.9233, dtype=torch.float64)\n",
            "31300 tensor(632.1529, dtype=torch.float64)\n",
            "31400 tensor(630.9402, dtype=torch.float64)\n",
            "31500 tensor(629.7812, dtype=torch.float64)\n",
            "31600 tensor(627.4311, dtype=torch.float64)\n",
            "31700 tensor(626.4047, dtype=torch.float64)\n",
            "31800 tensor(624.9005, dtype=torch.float64)\n",
            "31900 tensor(623.9122, dtype=torch.float64)\n",
            "32000 tensor(622.5923, dtype=torch.float64)\n",
            "32100 tensor(620.3458, dtype=torch.float64)\n",
            "32200 tensor(619.5290, dtype=torch.float64)\n",
            "32300 tensor(618.0768, dtype=torch.float64)\n",
            "32400 tensor(617.8067, dtype=torch.float64)\n",
            "32500 tensor(615.5838, dtype=torch.float64)\n",
            "32600 tensor(613.8817, dtype=torch.float64)\n",
            "32700 tensor(613.1403, dtype=torch.float64)\n",
            "32800 tensor(611.0962, dtype=torch.float64)\n",
            "32900 tensor(609.8005, dtype=torch.float64)\n",
            "33000 tensor(608.6645, dtype=torch.float64)\n",
            "33100 tensor(606.9697, dtype=torch.float64)\n",
            "33200 tensor(606.1117, dtype=torch.float64)\n",
            "33300 tensor(605.0462, dtype=torch.float64)\n",
            "33400 tensor(603.6726, dtype=torch.float64)\n",
            "33500 tensor(602.7836, dtype=torch.float64)\n",
            "33600 tensor(600.5148, dtype=torch.float64)\n",
            "33700 tensor(599.9624, dtype=torch.float64)\n",
            "33800 tensor(598.1783, dtype=torch.float64)\n",
            "33900 tensor(597.5261, dtype=torch.float64)\n",
            "34000 tensor(596.0991, dtype=torch.float64)\n",
            "34100 tensor(594.2934, dtype=torch.float64)\n",
            "34200 tensor(593.7055, dtype=torch.float64)\n",
            "34300 tensor(592.3395, dtype=torch.float64)\n",
            "34400 tensor(591.2654, dtype=torch.float64)\n",
            "34500 tensor(590.5715, dtype=torch.float64)\n",
            "34600 tensor(588.2525, dtype=torch.float64)\n",
            "34700 tensor(588.0576, dtype=torch.float64)\n",
            "34800 tensor(586.2943, dtype=torch.float64)\n",
            "34900 tensor(584.9399, dtype=torch.float64)\n",
            "35000 tensor(584.1151, dtype=torch.float64)\n",
            "35100 tensor(582.3475, dtype=torch.float64)\n",
            "35200 tensor(582.2938, dtype=torch.float64)\n",
            "35300 tensor(580.2405, dtype=torch.float64)\n",
            "35400 tensor(579.0236, dtype=torch.float64)\n",
            "35500 tensor(578.4887, dtype=torch.float64)\n",
            "35600 tensor(576.7137, dtype=torch.float64)\n",
            "35700 tensor(576.5606, dtype=torch.float64)\n",
            "35800 tensor(574.8243, dtype=torch.float64)\n",
            "35900 tensor(573.5194, dtype=torch.float64)\n",
            "36000 tensor(572.6914, dtype=torch.float64)\n",
            "36100 tensor(571.5775, dtype=torch.float64)\n",
            "36200 tensor(571.0229, dtype=torch.float64)\n",
            "36300 tensor(569.0248, dtype=torch.float64)\n",
            "36400 tensor(568.3725, dtype=torch.float64)\n",
            "36500 tensor(567.0240, dtype=torch.float64)\n",
            "36600 tensor(565.5148, dtype=torch.float64)\n",
            "36700 tensor(564.8140, dtype=torch.float64)\n",
            "36800 tensor(563.9423, dtype=torch.float64)\n",
            "36900 tensor(562.7370, dtype=torch.float64)\n",
            "37000 tensor(561.8292, dtype=torch.float64)\n",
            "37100 tensor(560.3182, dtype=torch.float64)\n",
            "37200 tensor(559.5182, dtype=torch.float64)\n",
            "37300 tensor(558.4959, dtype=torch.float64)\n",
            "37400 tensor(557.5786, dtype=torch.float64)\n",
            "37500 tensor(556.5720, dtype=torch.float64)\n",
            "37600 tensor(555.2170, dtype=torch.float64)\n",
            "37700 tensor(555.4734, dtype=torch.float64)\n",
            "37800 tensor(553.3961, dtype=torch.float64)\n",
            "37900 tensor(552.7032, dtype=torch.float64)\n",
            "38000 tensor(551.7841, dtype=torch.float64)\n",
            "38100 tensor(550.6177, dtype=torch.float64)\n",
            "38200 tensor(549.6992, dtype=torch.float64)\n",
            "38300 tensor(548.5056, dtype=torch.float64)\n",
            "38400 tensor(547.4511, dtype=torch.float64)\n",
            "38500 tensor(546.5116, dtype=torch.float64)\n",
            "38600 tensor(545.1127, dtype=torch.float64)\n",
            "38700 tensor(544.9515, dtype=torch.float64)\n",
            "38800 tensor(543.9968, dtype=torch.float64)\n",
            "38900 tensor(542.6516, dtype=torch.float64)\n",
            "39000 tensor(541.6165, dtype=torch.float64)\n",
            "39100 tensor(540.3625, dtype=torch.float64)\n",
            "39200 tensor(539.8232, dtype=torch.float64)\n",
            "39300 tensor(538.9048, dtype=torch.float64)\n",
            "39400 tensor(537.7891, dtype=torch.float64)\n",
            "39500 tensor(536.9258, dtype=torch.float64)\n",
            "39600 tensor(535.6422, dtype=torch.float64)\n",
            "39700 tensor(535.4675, dtype=torch.float64)\n",
            "39800 tensor(533.9042, dtype=torch.float64)\n",
            "39900 tensor(533.2333, dtype=torch.float64)\n",
            "40000 tensor(532.3384, dtype=torch.float64)\n",
            "40100 tensor(530.9974, dtype=torch.float64)\n",
            "40200 tensor(530.3902, dtype=torch.float64)\n",
            "40300 tensor(529.6260, dtype=torch.float64)\n",
            "40400 tensor(528.5080, dtype=torch.float64)\n",
            "40500 tensor(527.9722, dtype=torch.float64)\n",
            "40600 tensor(526.6630, dtype=torch.float64)\n",
            "40700 tensor(525.8996, dtype=torch.float64)\n",
            "40800 tensor(525.1790, dtype=torch.float64)\n",
            "40900 tensor(524.6483, dtype=torch.float64)\n",
            "41000 tensor(523.5517, dtype=torch.float64)\n",
            "41100 tensor(522.3402, dtype=torch.float64)\n",
            "41200 tensor(521.6551, dtype=torch.float64)\n",
            "41300 tensor(520.8673, dtype=torch.float64)\n",
            "41400 tensor(519.9071, dtype=torch.float64)\n",
            "41500 tensor(519.1277, dtype=torch.float64)\n",
            "41600 tensor(517.9236, dtype=torch.float64)\n",
            "41700 tensor(517.4725, dtype=torch.float64)\n",
            "41800 tensor(516.6179, dtype=torch.float64)\n",
            "41900 tensor(515.9961, dtype=torch.float64)\n",
            "42000 tensor(515.0702, dtype=torch.float64)\n",
            "42100 tensor(513.7086, dtype=torch.float64)\n",
            "42200 tensor(513.5108, dtype=torch.float64)\n",
            "42300 tensor(512.3737, dtype=torch.float64)\n",
            "42400 tensor(511.7627, dtype=torch.float64)\n",
            "42500 tensor(511.1612, dtype=torch.float64)\n",
            "42600 tensor(509.6090, dtype=torch.float64)\n",
            "42700 tensor(509.4575, dtype=torch.float64)\n",
            "42800 tensor(508.1642, dtype=torch.float64)\n",
            "42900 tensor(507.4077, dtype=torch.float64)\n",
            "43000 tensor(506.8814, dtype=torch.float64)\n",
            "43100 tensor(505.5307, dtype=torch.float64)\n",
            "43200 tensor(505.1978, dtype=torch.float64)\n",
            "43300 tensor(504.5448, dtype=torch.float64)\n",
            "43400 tensor(503.5929, dtype=torch.float64)\n",
            "43500 tensor(502.7521, dtype=torch.float64)\n",
            "43600 tensor(501.8276, dtype=torch.float64)\n",
            "43700 tensor(501.1902, dtype=torch.float64)\n",
            "43800 tensor(500.2510, dtype=torch.float64)\n",
            "43900 tensor(499.5643, dtype=torch.float64)\n",
            "44000 tensor(498.8763, dtype=torch.float64)\n",
            "44100 tensor(497.8439, dtype=torch.float64)\n",
            "44200 tensor(497.6209, dtype=torch.float64)\n",
            "44300 tensor(496.4327, dtype=torch.float64)\n",
            "44400 tensor(496.2472, dtype=torch.float64)\n",
            "44500 tensor(495.0167, dtype=torch.float64)\n",
            "44600 tensor(494.0040, dtype=torch.float64)\n",
            "44700 tensor(493.4455, dtype=torch.float64)\n",
            "44800 tensor(492.8524, dtype=torch.float64)\n",
            "44900 tensor(492.0786, dtype=torch.float64)\n",
            "45000 tensor(491.4516, dtype=torch.float64)\n",
            "45100 tensor(490.3696, dtype=torch.float64)\n",
            "45200 tensor(490.3227, dtype=torch.float64)\n",
            "45300 tensor(489.3818, dtype=torch.float64)\n",
            "45400 tensor(488.3387, dtype=torch.float64)\n",
            "45500 tensor(487.8846, dtype=torch.float64)\n",
            "45600 tensor(486.6143, dtype=torch.float64)\n",
            "45700 tensor(486.6759, dtype=torch.float64)\n",
            "45800 tensor(485.4202, dtype=torch.float64)\n",
            "45900 tensor(485.0671, dtype=torch.float64)\n",
            "46000 tensor(484.6987, dtype=torch.float64)\n",
            "46100 tensor(483.3209, dtype=torch.float64)\n",
            "46200 tensor(483.2943, dtype=torch.float64)\n",
            "46300 tensor(482.0443, dtype=torch.float64)\n",
            "46400 tensor(481.1481, dtype=torch.float64)\n",
            "46500 tensor(480.7956, dtype=torch.float64)\n",
            "46600 tensor(479.6433, dtype=torch.float64)\n",
            "46700 tensor(479.9254, dtype=torch.float64)\n",
            "46800 tensor(478.4624, dtype=torch.float64)\n",
            "46900 tensor(477.9019, dtype=torch.float64)\n",
            "47000 tensor(477.4705, dtype=torch.float64)\n",
            "47100 tensor(476.1741, dtype=torch.float64)\n",
            "47200 tensor(475.8378, dtype=torch.float64)\n",
            "47300 tensor(474.9644, dtype=torch.float64)\n",
            "47400 tensor(474.4979, dtype=torch.float64)\n",
            "47500 tensor(473.9692, dtype=torch.float64)\n",
            "47600 tensor(472.7489, dtype=torch.float64)\n",
            "47700 tensor(472.5306, dtype=torch.float64)\n",
            "47800 tensor(471.7787, dtype=torch.float64)\n",
            "47900 tensor(471.2414, dtype=torch.float64)\n",
            "48000 tensor(470.6150, dtype=torch.float64)\n",
            "48100 tensor(469.5916, dtype=torch.float64)\n",
            "48200 tensor(469.1738, dtype=torch.float64)\n",
            "48300 tensor(468.4205, dtype=torch.float64)\n",
            "48400 tensor(467.7946, dtype=torch.float64)\n",
            "48500 tensor(467.5320, dtype=torch.float64)\n",
            "48600 tensor(466.2652, dtype=torch.float64)\n",
            "48700 tensor(466.1488, dtype=torch.float64)\n",
            "48800 tensor(465.1890, dtype=torch.float64)\n",
            "48900 tensor(465.0422, dtype=torch.float64)\n",
            "49000 tensor(464.3919, dtype=torch.float64)\n",
            "49100 tensor(463.2646, dtype=torch.float64)\n",
            "49200 tensor(462.9679, dtype=torch.float64)\n",
            "49300 tensor(462.0354, dtype=torch.float64)\n",
            "49400 tensor(461.3758, dtype=torch.float64)\n",
            "49500 tensor(461.0430, dtype=torch.float64)\n",
            "49600 tensor(459.9993, dtype=torch.float64)\n",
            "49700 tensor(460.0578, dtype=torch.float64)\n",
            "49800 tensor(458.8858, dtype=torch.float64)\n",
            "49900 tensor(458.3578, dtype=torch.float64)\n",
            "50000 tensor(457.7750, dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "eta = .005  # learning rate = 0.002\n",
        "d = X_train.shape[1]\n",
        "batch_size = 20\n",
        "w_init = torch.randn(d,dtype=torch.float64)  # lấy random một vector w\n",
        "w, loss_list, count_list = logistic_sigmoid_regression(X_train, y_train, w_init, eta, batch_size)\n",
        "w = w.reshape((1,-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss = []\n",
        "for i in range(1,len(loss_list)):\n",
        "    loss.append(loss_list[i].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "Q3DHc2zDZ9p-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x1c001fc8f10>]"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(count_list,loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "XlsI65CpZ9p-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "315\n",
            "500\n",
            "0.63\n"
          ]
        }
      ],
      "source": [
        "predict = sigmoid(torch.matmul(w,torch.transpose(X_test,0,1)))\n",
        "count = 0\n",
        "for i in range(0,len(predict[0])):\n",
        "    x = predict[0][i]\n",
        "    y = y_test[i]\n",
        "    s = x + y\n",
        "    if (x + y >=1.5) or (x + y <0.5):\n",
        "        count+=1\n",
        "    #print(\"%.4f\" %predict[0][i],y_test[i])\n",
        "print(count)\n",
        "print(len(y_test))\n",
        "print(count/len(y_test))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "6_logicstics_labelRU_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
